---
title: "Prior Analysis Jose's Data and Posteriors"
author: "Matt Wheeler"
date: "9/22/2020"
output:
  html_document:
    html_document: null
    toc: yes
    toc_float: yes
  word_document: default
  pdf_document: default
---

```{r setup,include=FALSE}
library(ToxicR)
library(plot3D)
knitr::opts_chunk$set(echo = TRUE)
mData <- matrix(c(0, 1,10,
                  0.3, 4,10,
                  1, 4,10,
                  4, 7,10),nrow=4,ncol=3,byrow=T)
mData[,1] = mData[,1]/4
```

## Sensitivity Analysis of priors
I am going to look at the different priors and the resulting likelihood
that results for the following data-set using maximum likelihood estimation. 
Note that I have made all of the doses on a (0,1) scale for more appropriate 
comparisons. 

```{r data}
print(mData)
H = single_dichotomous_fit(mData[,1],mData[,2],mData[,3],model_type ="weibull",fit_type = "mle")
H$parameters
```

The default MLE for the $\alpha$ parameter hits its natural bound at
1.0. If we look at the likelihood without the beta we have the following
surface: 

```{r surface,echo=FALSE}
l_alpha <- seq(0.3,2.0,0.01)
l_gamma <- seq(0.2,1.5,0.01)
gamma <- matrix(0,nrow=length(l_gamma),ncol=length(l_alpha))
alpha <- matrix(0,nrow=length(l_gamma),ncol=length(l_alpha))

for (ii in 1:length(l_gamma)){
  for (jj in 1:length(l_alpha)){
    gamma[ii,jj] = l_gamma[ii]
    alpha[ii,jj] = l_alpha[jj]
  }
}

log_binom <-function(gamma,beta,alpha){
  
  dose <- c(0,0.3,1,4)
  obs  <- c(1,4,4,7)
  N    <- c(10,10,10,10)
  
  gamma2 <- 1/(1+exp(-gamma))
  P <- array(0,dim=c(nrow(alpha),ncol(alpha),length(dose)))
  L <- array(0,dim=c(nrow(alpha),ncol(alpha),length(dose)))
  for (ii in 1:length(dose)){
      P[,,ii] <- gamma2 + (1-gamma2)*(1-exp(-beta*dose[ii]^alpha))
      L[,,ii] <- obs[ii]*log(P[,,ii]) + (N[ii]-obs[ii])*log(1-P[,,ii])
  }
  
  rL <- matrix(0,nrow=nrow(alpha),ncol=ncol(alpha))
  for (ii in 1:length(dose)){
    rL = rL + L[,,ii]
  }
  return(rL)
}

t = log_binom(-2.16,gamma,alpha)
scatter3D(alpha,gamma,t)
```

So what happens when we start to mess around with the prior? Well 
lets see the default prior in `ToxicR' using MCMC for pretty plots
we have: 

```{r default prior}
H = single_dichotomous_fit(mData[,1],mData[,2],mData[,3],model_type = "weibull",fit_type = "mcmc")
print( H$fitted_model$parameters)
plot(H)
H$prior
```
Now the parameters are essentially no different than the estimate using 
maximum likelihood, but the experimental test prior, which is different than 
BMDS is pretty flat. What does the optimization surface look like? 

```{r default prior 2,echo=FALSE}
prior1 <-function(gamma,beta,alpha){
  a  <- dnorm(gamma,0,2,log=TRUE)
  b  <- dlnorm(alpha,0.42,0.5,log=TRUE)
  c  <- dlnorm(beta,0,1.5,log=TRUE)
 return(a+b+c)
}

pr <- prior1(-2.16,gamma,alpha)
lt <- pr + t
scatter3D(alpha,gamma,lt)
```
The peak is not noticeably different, so it is essentially performing the 
same inference. 

### Jose's First Prior
```{r new prior 1,echo=TRUE}
prior <- create_prior_list(normprior(0,2,-20,20),
                           lnormprior(0.5,0.3,0,20),
                           lnormprior(0,1.5,0,10000))
H = single_dichotomous_fit(mData[,1],mData[,2],mData[,3],model_type =   "weibull",fit_type = "mcmc",prior = prior)
print( H$fitted_model$parameters)
plot(H)
H$prior
```
Here the alpha kicks up to $1.44$ thus our prior is starting to impact our fit.  What 
does this prior surface look like? 
```{r new prior 1 plot2,echo=FALSE}
prior1 <-function(gamma,beta,alpha){
  a  <- dnorm(gamma,0,2,log=TRUE)
  b  <- dlnorm(alpha,0.5,0.2,log=TRUE)
  c  <- dlnorm(beta,0,1.5,log=TRUE)
 return(a+b+c)
}

pr <- prior1(-2.16,gamma,alpha)
lt <- pr + t
scatter3D(alpha,gamma,lt)
scatter3D(alpha,gamma,t)
```
Here the top plot is the 'new prior' and the bottom plot is the likelihood, and this 
side-by-side view starts to show us what is going on.  The bottom plot has no more than
a 0.5 change in the log likelihood between the MLE and the edge of the graph on the x
axis which is the $\alpha$ shape parameter.  The change is really insignificant, and the 
prior is thus informative increasing $\alpha$


### Jose's Second prior

Looking at the second prior on the list we can do the same as above. 

```{r new prior 2,echo=TRUE}
prior <- create_prior_list(normprior(0,2,-20,20),
                           lnormprior(0.8,0.5,0,20),
                           lnormprior(0,1.5,0,10000))
H = single_dichotomous_fit(mData[,1],mData[,2],mData[,3],model_type =   "weibull",fit_type = "mcmc",prior = prior)
print( H$fitted_model$parameters)
plot(H)
H$prior
```
```{r new prior 2 plot2,echo=FALSE}
prior1 <-function(gamma,beta,alpha){
  a  <- dnorm(gamma,0,2,log=TRUE)
  b  <- dlnorm(alpha,0.8,0.5,log=TRUE)
  c  <- dlnorm(beta,0,1.5,log=TRUE)
 return(a+b+c)
}

pr <- prior1(-2.16,gamma,alpha)
lt <- pr + t
scatter3D(alpha,gamma,lt)
scatter3D(alpha,gamma,t)
```
 Given there is more variance the ellipsis/quadratic does not move as far but it still 
 moves.  The problem is the same as above. There is barely any difference between an $\alpha = 1$
 and an $\alpha = 1.4$ as far as inference is concerned. 
 
### Jose's Third prior

Looking at the second prior on the list we can do the same as above. 

```{r new prior 3,echo=TRUE}
prior <- create_prior_list(normprior(0,2,-20,20),
                           lnormprior(-1,0.2,0,20),
                           lnormprior(0,1.5,0,10000))
H = single_dichotomous_fit(mData[,1],mData[,2],mData[,3],model_type =   "weibull",fit_type = "mcmc",prior = prior)
print( H$fitted_model$parameters)
plot(H)
H$prior
```
```{r new prior 3 plot2,echo=FALSE}
prior1 <-function(gamma,beta,alpha){
  a  <- dnorm(gamma,0,2,log=TRUE)
  b  <- dlnorm(alpha,-1,0.2,log=TRUE)
  c  <- dlnorm(beta,0,1.5,log=TRUE)
 return(a+b+c)
}

pr <- prior1(-2.16,gamma,alpha)
lt <- pr + t
scatter3D(alpha,gamma,lt)
scatter3D(alpha,gamma,t)
```

Now this prior is wacky, but it gives the result we would expect.  It puts all 
of the mass of $\alpha$ below 1 and give a nutjob (scientific term) surface to 
optimize. 

### BMDS Standard Prior 

```{r new prior 4,echo=TRUE}
prior <- create_prior_list(normprior(0,2,-20,20),
                           lnormprior(0.69,0.42,0,20),
                           lnormprior(0,1.5,0,10000))
H = single_dichotomous_fit(mData[,1],mData[,2],mData[,3],model_type =   "weibull",fit_type = "mcmc",prior = prior)
print( H$fitted_model$parameters)
plot(H)
H$prior
```
```{r new prior 4 plot2,echo=FALSE}
prior1 <-function(gamma,beta,alpha){
  a  <- dnorm(gamma,0,2,log=TRUE)
  b  <- dlnorm(alpha,0.69,0.42,log=TRUE)
  c  <- dlnorm(beta,0,1.5,log=TRUE)
 return(a+b+c)
}

pr <- prior1(-2.16,gamma,alpha)
lt <- pr + t
scatter3D(alpha,gamma,lt)
scatter3D(alpha,gamma,t)
```

Again, we see that the default BMDS prior puts things in a much more 'reasonable'
area, because truth be told there is not a heck of a lot of information to put $\alpha$
below zero. In fact a likelihood difference of 0.5 is barely a sneeze.  To me, 
this just shows how little information can be in these data sets that inform our
models.  As always, being careful is the name of the game.  

### Continuous Model Averaging
Jay here is Continuous MA
```{r Cont Model Averaging}
library(ToxicR)

set.seed(893223)

D <-c(rep(seq(0,1.0,1/4),each=4))
mean <- 2.3  + 10/(1+exp(-(D-0.60)*8))*(1/(1+exp(-(0.99-D)*13)))

Y <- mean + rnorm(length(mean),0,0.7)
#Q <- single_continuous_fit(as.matrix(D),as.matrix(Y),sstat = F,BMR = 1.0 ,model_type="FUNL",distribution = "normal",fit_type = "laplace")

system.time({fit<-ma_continuous_fit(D,Y,fit_type="mcmc",samples=25000,burnin=2500,BMR=2.0)})
system.time({fit2<-ma_continuous_fit(D,Y,fit_type="laplace",BMR=2.0)})
plot(fit)
```
